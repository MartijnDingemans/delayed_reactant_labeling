Optimize
========

To optimize the rate constants, a model needs to be created that can create a prediction of the concentrations, and
can subsequently calculate the corresponding errors. This model can than:

1. Create an a guess of the rate constants.
2. Compare the result of the prediction with the experimental data
3. Weighing the errors
4. Save the intermediate results
5. It will than repeat 1-4 until with different rate constants until the error no longer improves and the rate constants have converged to a stable value or the maximum number of iterations has been reached. The method, that is used to find the next guess, is the `Nelder-Mead <https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html>`_ algorithm with adaptive parameters.

The :class:`RateConstantOptimizerTemplate` is an abstract base class which has implemented the numbered steps
above. However, the user must define the error functions and the exact methodology of creating a prediction. The definition
of the error functions consists out of two parts. First the data (either experimental or predicted) must be converted to
a curve of interest, such as: :math:`A / (A + A_{labeled})`. Subsequently a ``metric`` wil be used to evaluate the
error between each curve of both datasets.

.. py:currentmodule:: optimize

.. autoclass:: RateConstantOptimizerTemplate
    :members:

.. autoclass:: OptimizedModel
    :members:

.. autoclass:: OptimizedMultipleModels
    :members:

.. _OptimizeExample:

Example
-------
To optimize the rate constants we will analyze a simple system. We can describe the model and the experimental
conditions as follows:

.. math::

    \require{mhchem}
    \ce{catalyst + A <=>[\ce{k1}][\ce{k_{-1}}] B ->[\ce{{k2}}] C + catalyst}

.. code-block:: python

    import numpy as np
    reactions = [
        ('k1', ['A', 'cat'], ['B'],),
        ('k-1', ['B'], ['A', 'cat'],),
        ('k2', ['B'], ['C', 'cat']),

        # labeled
        ('k1', ['A-d10', 'cat'], ['B-d10'],),
        ('k-1', ['B-d10'], ['A-d10', 'cat'],),
        ('k2', ['B-d10'], ['C-d10', 'cat'])
    ]

    # look at as simple of a system as possible.
    concentration_initial = {'A': 1, 'cat': 1 / 5}
    concentration_labeled = {'A-d10': 1}
    dilution_factor = 1
    time_pre = np.linspace(0, 10, 50)
    time_post = np.linspace(10, 90, 8 * 50)

To be able to fit to this hypothetical reaction fake data was generated by first creating a prediction using the :class:`DRL <predict.DRL>`
class. To this data we than add noise based on its intensity and a base level of noise to mimic real experimental data.

.. code-block:: python

    from delayed_reactant_labeling.predict import DRL
    import pandas as pd

    # create a "real" prediction.
    rate_constants_real = {'k1': 0.3, 'k-1': 0.05, 'k2': 0.5}
    drl_real = DRL(rate_constants=rate_constants_real, reactions=reactions)
    real_data = drl_real.predict_concentration(
        t_eval_pre=time_pre,
        t_eval_post=time_post,
        dilution_factor=dilution_factor,
        initial_concentrations=concentration_initial,
        labeled_concentration=concentration_labeled)

    # add noise
    rng = np.random.default_rng(42)
    fake_data = []
    for col in real_data.columns[:-1]:  # the last column contains a time array, so skip that one.
        noise_dynamic = rng.normal(loc=1, scale=0.05, size=real_data[col].shape)  # fraction error
        noise_static = rng.normal(loc=0.015, scale=0.0075, size=real_data[col].shape)
        fake_col = real_data[col] * noise_dynamic + noise_static
        fake_col[fake_col < 1e-10] = 1e-10  # no negative intensity
        fake_data.append(fake_col)
    fake_data.append(real_data['time'])
    fake_data = pd.DataFrame(fake_data, index=real_data.columns).T

    # visualize
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots()
    for col in fake_data.columns[:-1]:
        ax.plot(time_post, real_data[col], color="tab:gray")
        ax.scatter(time_post, fake_data[col], label=col)
    ax.plot(np.nan, np.nan, color="tab:gray", label="real")
    ax.set_xlabel("time")
    ax.set_ylabel("intensity")
    ax.legend()
    fig.show()

.. image:: images/optimize_fakedata.png
    :width: 600
    :align: center

The :class:`RateConstantOptimizerTemplate` contains several functions which allow it to optimize a model.
However, it does require the user to define two functions. The first :meth:`create_prediction <RateConstantOptimizerTemplate.create_prediction>`
should tell the class how exactly it can create a prediction from a given set of parameters.
This function allows the user also to modify the parameters its given, or to modify the output of the prediction.
The second function :meth:`calculate_curves <RateConstantOptimizerTemplate.calculate_curves>` describes how
the data should be analyzed.

.. code-block:: python

    from __future__ import annotations  # required for compatibility with python 3.8
    from delayed_reactant_labeling.predict import DRL
    from delayed_reactant_labeling.optimize import RateConstantOptimizerTemplate

    class RateConstantOptimizer(RateConstantOptimizerTemplate):
        @staticmethod
        def create_prediction(x: np.ndarray, x_description: list[str]) -> pd.DataFrame:
            rate_constants = pd.Series(x, x_description)
            # The rate constants can easily be manipulated here. For example,
            # rate_constants["k1"] = 0.42 would fixate that value.
            # Because Series are mutable, the changed version will be stored in the logs!

            drl = DRL(reactions=reactions, rate_constants=rate_constants)
            pred_labeled = drl.predict_concentration(
                t_eval_pre=time_pre,
                t_eval_post=time_post,
                initial_concentrations=concentration_initial,
                labeled_concentration=concentration_labeled,
                dilution_factor=dilution_factor,
                rtol=1e-8,
                atol=1e-8, )

            # The prediction can be altered here before its analyzed.
            return pred_labeled

        @staticmethod
        def calculate_curves(data: pd.DataFrame) -> dict[str, np.ndarray]:
            curves = {}
            for chemical in ['A', 'B', 'C']:
                chemical_sum = data[[chemical, f'{chemical}-d10']].sum(axis=1)
                curves[f'ratio_{chemical}'] = (data[chemical] / chemical_sum).to_numpy()
            return curves

Internally the class compares the curves of the predicted data with the experimental data using a metric function. The
function that calculates the Mean Absolute Error can for example be defined as follows:

.. code-block:: python

    def metric(y_true, y_pred):
        return np.average( np.abs(y_true - y_pred))

The `np.average <https://numpy.org/doc/stable/reference/generated/numpy.average.html#numpy-average>`_ function also
takes a weight keyword, so this can easily be implemented into the metric. Other functions such as
`np.nanmean <https://numpy.org/doc/stable/reference/generated/numpy.nanmean.html#numpy-nanmean>`_ can be used to skip
NaN values. `Scikit-learn.metrics <https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics>`_
implements a lot of different metrics and is a great resource. However, their functions also always check the arguments very
precisely, which can lead to significant slow-downs.

To start optimizing this system we do the following:

.. code-block:: python

    from scipy.optimize import Bounds
    description = ['k1', 'k-1', 'k2']
    bounds = Bounds(np.array([1e-9, 1e-9, 1e-9]), np.array([100, 100, 100]))  #lower bound, upper bound

    RCO = RateConstantOptimizer(experimental=fake_data, metric=metric)
    RCO.optimize(
        x0=np.array([1, 1, 1]),
        x_description=description,
        x_bounds=bounds,
        path='./optimization/', _overwrite_log=True)

    model = RCO.load_optimized_model('./optimization/')

    model.best_X
    # k1,   2.112804e-01
    # k-1,  1.000000e-09
    # k2,   6.424953e-01

However, the results of a optimization run are heavily dependent on its starting position, especially for systems
with many different reactions. To analyze multiple runs from different starting positions we can use:

.. code-block:: python

    RCO.optimize_multiple(
        path='./optimization_multiple/',
        n_runs=50,
        x_description=description,
        x_bounds=bounds,
        n_jobs=-2,  # use all cpu's except one.
    )

    from delayed_reactant_labeling.optimize import OptimizedMultipleModels
    models = OptimizedMultipleModels(path='./optimization_multiple/')
    print(models.best.optimal_x)
    # k1     2.112738e-01
    # k-1    1.000000e-09
    # k2     6.425392e-01
